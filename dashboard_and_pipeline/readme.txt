


1. После восстановления базы данных zen из дампа и создания таблиц dash_visits и dash_engagement, необходимо запустить zen_pipeline для сбора данных в созданные таблицы.
Мы не используем cron + у нас данные всего за один день, поэтому не будем писать для него расписание.
Чтобы посмотреть какие таблицы у нас есть в базе данных, в терминале можно набрать команду \dt, она покажет все созданные таблицы.


2. Далее необходимо запустить pipeline с входными параметрами. В моем пути до файла это выглядит таким образом:
path - путь до файла

python3 /path/zen_pipeline.py --start_dt='2019-09-24 18:00:00' --end_dt='2019-09-24 19:00:00'



3. Далее хорошо бы проверить, появились ли данные таблицах dash_visits и dash_engagement в базе данных zen. Это можно сделать так (после входа в postgres):
 

 SELECT COUNT(*) FROM dash_visits;
 SELECT * FROM dash_visits LIMIT 20;

 SELECT COUNT(*) FROM dash_engagement;
 SELECT * FROM dash_engagement LIMIT 20;


4. Если данные на месте, можно запустить из под основного юзера скрипт дашборда, для этого в терминале пишем(путь меняем на свой):
path - путь до файла

python3 /path/zen_dashboard.py

5. Копируем полученную ссылку на локалхост и заходим в бразере, наслаждаемся.  


6. Создаем дамп

---Раздаем права
path - путь
folder - папка на которую выдаем полные права

sudo chmod -R 777 /Users/peter/Desktop/  (в моем случае это выглядит так)
sudo chmod -R 777 /path/folder (в общем случае это выглядит так)

--Подключаемся под пользователя постгрес
sudo su postgres

--Делаем дамп
path - полный путь, куда мы хотим сохранить файл

pg_dump -Fc zen > /path/zen_export.dump
